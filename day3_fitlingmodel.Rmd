---
title: "day3_fitlingmodel"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introducing likelihood functions

In Gadget, a likelihood function is specified by supplying likelihood components within the likelihood
file, which is referred to by the main file. Individual likelihood component scores are summed together 
after being multiplied by weights to form a total likelihood score, and likelihood scores are defined by the type of likelihood component specified. 
Component types define both the kind of data and likelihood function being calculated for that component.
Although we refer to 'likelihood scores' throughout this description, it should be kept in mind that these
are more accurately sums of log likelihoods, so that the objective function used for model fitting attempts to minimize the total likelihood score.


### Incorporating data (Ling case study cont.)
In introducing likelihood components, we will first go through those used within the ling example in detail, but only mention the other types as they follow similar patterns of implementation. To use Rgadget to set up likelihood files, the first consideration that should be made is what data are available that would be useful for creating a likelihood component. These have been provided for you here: in the data_provided folder, you will find catch data (as introduced with the fleets), as well as survey indices, catch distribution, and 'stock' distribution data. Catch distribution include numbers at length as well as numbers at age and length by fleet. Each length- or age-bin has been defined as having the lower bound only inclusive, except that the lower bound of the smallest bin and the upper bound of the largest bin are open-ended to include all values lower or higher, respectively. These numbers are then used within Gadget to form proportions among defined bins, which are compared to the same proportions among the same bins simulated in the model. Stock distribution data are numbers within each stock, which for ling is immature or mature, but could for example be gender-specific stocks. These are similarly used to form proportions within Gadget.

As we saw when incorporating catch data into fleet files (day 2), certain attributes need to be present 
in order for the proper and internally consistent Gadget aggregation files to be created alongside these data files. For catch and stock distribution data, these include area, length, and age aggregations. Even when there is only a single bin, these aggregations need to be specified. For example a single area and a single age group is needed for length distributions, in addition to the length bins. Note that bins need not be evenly distributed across the total length range; instead they are highly flexible and can be specified as any grouping. Therefore the user should be careful to ensure that the definitions make sense and that bins do not overlap. In our example below, we show how to add those necessary attributes, although in reality to do this we also use another R package named 'mfdb' that is aimed at database management and data extraction. This package will be briefly introduced on the last day
of this class but is generally beyond the scope of this course.

```{r likelihood}

library(Rgadget)

#assuming you are beginning in a fresh session; this information should be in an 
#initialization script

base_dir <- 'ling_model'
vers <- c('01-base')
gd <- gadget.variant.dir(sprintf(paste0("%s/",vers),base_dir))

ling.imm <- read.gadget.file(gd, 'lingimm')
ling.mat <- read.gadget.file(gd, 'lingmat')
area_file <- read.gadget.file(gd, 'Modelfiles/area')

minage <- ling.imm[[1]]$minage
maxage <- ling.mat[[1]]$maxage
maxlength <- ling.mat[[1]]$maxlength 
minlength <- ling.imm[[1]]$minlength
dl <- ling.imm[[1]]$dl

create_intervals <-  function (prefix, vect) {
  
    x<-
    structure(vect[1:(length(vect)-1)], names = paste0(prefix, vect[1:(length(vect)-1)])) %>% 
    as.list(.) %>% 
    purrr::map(~structure(., 
                          min = ., 
                          max = vect[-1][which(vect[1:(length(vect)-1)]==.)]))
    
    return(x)
}
#just to see what this function does:
create_intervals('len',seq(minlength, maxlength, dl)) %>% .[1:3]
```

```{r}
#catch distribution data available:
  #length distributions do not regard age bins
ldist.surv <- structure(read_csv('data_provided/ldist_sur.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len',seq(minlength, maxlength, dl)),
            age = create_intervals('all',seq(minage, maxage, maxage-minage))
            )
ldist.lln <- structure(read_csv('data_provided/ldist_lln.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len',seq(minlength, maxlength, dl)),
            age = create_intervals('all',seq(minage, maxage, maxage-minage))
            )
ldist.bmt <- structure(read_csv('data_provided/ldist_bmt.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len',seq(minlength, maxlength, dl)),
            age = create_intervals('all',seq(minage, maxage, maxage-minage))
            )
ldist.gil <- structure(read_csv('data_provided/ldist_gil.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len',seq(minlength, maxlength, dl)),
            age = create_intervals('all',seq(minage, maxage, maxage-minage))
            )

attributes(ldist.surv)

```

```{r}

#age bins needed for age-length distributions
aldist.surv <- structure(read_csv('data_provided/aldist_sur.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len',seq(minlength, maxlength, dl)),
            age = create_intervals('age',c(minage:11, maxage))
            )
aldist.lln <- structure(read_csv('data_provided/aldist_lln.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len',seq(minlength, maxlength, dl)),
            age = create_intervals('age',c(minage:11, maxage))
            )
aldist.bmt <- structure(read_csv('data_provided/aldist_bmt.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len',seq(minlength, maxlength, dl)),
            age = create_intervals('age',c(minage:11, maxage))
            )
aldist.gil <- structure(read_csv('data_provided/aldist_gil.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len',seq(minlength, maxlength, dl)),
            age = create_intervals('age',c(minage:11, maxage))
            )

attributes(aldist.surv)
```

```{r}

#stock distribution data available:
matp.surv <- structure(read_csv('data_provided/matp_sur.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len',seq(minlength, maxlength, dl*2)),
            age = list(mat_ages = ling.mat[[1]]$minage:ling.mat[[1]]$maxage)
            )

attributes(matp.surv)

```

For ling, length-based indices were used by choosing 7 'slices' of abundance indices across the
length range of ling observed. Index bounds are defined as above, generally inclusive of the lower bound only, but open-ended for the smallest and largest bins.
are all other length- and age- bins presented above). No age extrapolations are necessary. It is important to note that only area and length attributes are included as attributes on these data frames, rather than age as in the distributional components. The length attribute allows `gadget_update` to recognize these data as length-based indices, rather than age-based or acoustic indices, for example. It is also important to keep in mind
here that each slice will have its own estimated catchability per survey (single data frame below)
and area combination (multiple areas may be within a data frame). More on catchability later...

```{r}

#survey index data available: 
slices <- 
  list('len20' = c(20,52), 'len52' = c(52,60), 'len60' = c(60,72), 'len72' = c(72,80), 'len80' = c(80,92), 'len92' = c(92,100), 'len100' = c(100,160) )

# just to illustrate slices:

ldist.surv %>% mutate(length = substring(length, 4) %>% as.numeric) %>% group_by(length) %>% summarise(n = sum(number)) %>% ggplot(aes(x = length, y = n)) + geom_col() + geom_vline(aes(xintercept = length), lty = 2, col = 'orange', data = slices %>% bind_rows() %>% t() %>% .[-1,1] %>% tibble(length = .))
```

```{r}
# set up aggregation info for length-based survey indices
SI1 <- structure(read_csv('data_provided/SI1.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len', slices[[1]]))
SI2 <- structure(read_csv('data_provided/SI2.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len', slices[[2]]))
SI3 <- structure(read_csv('data_provided/SI3.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len', slices[[3]]))
SI4 <- structure(read_csv('data_provided/SI4.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len', slices[[4]]))
SI5 <- structure(read_csv('data_provided/SI5.csv'),
             area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len', slices[[5]]))
SI6 <- structure(read_csv('data_provided/SI6.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len', slices[[6]]))
SI7 <- structure(read_csv('data_provided/SI7.csv'),
            area = list(area_file[[1]]$areas) %>% set_names(.),
            length = create_intervals('len', slices[[7]]))

```


### Setting up likelihood components
Using Rgadget, writing the files that specify the likelihood componenets is straightforward. When calling `gadgetlikelihood` and then writing it with `write.gadget.file`, a likelihood file is created within R and written to disc.  The likelihood file contains a list of likelihood component specifications, and within this list is where the likelihood function, data, and weighting of the likelihood component is specifies.

Here we start with the distributional components. In all cases, each component receives a distinct name (to be referenced later so keep them short and distinct), weights (1 to yield equal weights for now), data as imported above, and the fleet names and stock names to which these data apply. As a default, Rgadget specifies a sum of squares function to calculate the likelihood score (`'sumofsquares'`), although a variety of other functions are possible, including one for stratified sum of squares (`'stratified'`), multinomial function (`'multinomial'`), pearson function (`'pearson'`), gamma function (`'gamma'`), log function (`'log'`), multivariate normal function (`'mvn'`) or a multivariate logistic function (`'mvlogistic'`). Many of these functions have been tailored for specific data situations (e.g., including tagging or predation data into a likelihood component). More on these functions can be read in the Gadget User Guide. To change the default setting to the stratified sum of squares, for example, one would include an additional argument `function = 'stratified',` to the `gadget_update` call. 

```{r}

lik <-
  gadgetlikelihood('likelihood',gd,missingOkay = TRUE) %>% 
  gadget_update("catchdistribution",
                name = "ldist.surv",
                weight = 1,
                data = ldist.surv,
                fleetnames = c("surv"),
                stocknames =c("lingimm","lingmat")) %>% 
  gadget_update("catchdistribution",
                name = "aldist.surv",
                weight = 1,
                data = aldist.surv, 
                fleetnames = c("surv"),
                stocknames =c("lingimm","lingmat")) %>% 
  gadget_update("catchdistribution",
                name = "ldist.lln",
                weight = 1,
                data = ldist.lln, 
                fleetnames = c("lln"),
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("catchdistribution",
                name = "aldist.lln",
                weight = 1,
                  data = aldist.lln,
                fleetnames = c("lln"),
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("catchdistribution",
                name = "ldist.gil",
                weight = 1,
                data = ldist.gil,
                fleetnames = c("gil"),
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("catchdistribution",
                name = "aldist.gil",
                weight = 1,
                data = aldist.gil,
                fleetnames = c("gil"),
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("catchdistribution",
                name = "ldist.bmt",
                weight = 1,
                data = ldist.bmt,
                fleetnames = c("bmt"),
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("catchdistribution",
                name = "aldist.bmt",
                weight = 1,
                data = aldist.bmt,
                fleetnames = c("bmt"),
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("stockdistribution",
                name = "matp.surv",
                weight = 1,
                  data = matp.surv,
                fleetnames = c("surv"),
                stocknames =c("lingimm","lingmat")) 

lik

```

Note here that Rgadget automatically creates references to necessary data files ('Data/...') and aggregation files ('Agg/...') that define the bin structure as defined by data attributes (see previous section). When `write.gadget.file` is finally called after we are done adding all likelihood components, these data and agg files will be written alongside this likelihood file, and a reference to this likelihood file will also be drawn within the 'main' file. 

Incorporating survey index components is very similar except that 1) the fact that the survey index is length-based is determined by the length attribute of the data frame, which sets the argument `si_type` to a value of `lengths`, and 2) a catchability function is also specified under the argument `'fittype'`. In addition, the argument `biomass` has a default setting of 0, signifying  a numbers-based index (rather than a biomass-based index, which has a value of 1). Other options for `si_type` can be read about in the Gadget User Guide, and `ages`, `fleets`, `acoustic`, and `effort`.

The catchability function defines the type of linear regression to be used to calculate the likelihood score. Options include a linear fit on the orignal or log scale (`'linearfit'` or `'loglinearfit'`), linear fit with a fixed slope on the original or log scale (`'fixedslopelinearfit'` or `'fixedslopeloglinearfit'`), linear fit with a fixed intercept on the original or log scale (`'fixedinterceptlinearfit'` or `'fixedinterceptlinearfit'`), or a linear/loglinear function with both the slope and intercept fixed (`'fixedlinearfit'` or `'fixedloglinearfit'`). In the first set of cases (linear or loglinear fits), two parameters would be estimated; in the next two sets of cases, only a single parameter is estimated (slope or intercept, whichever is not fixed). In the last set of cases (fixed liner or loglinear fits), no parameters are estimated. Whenever a parameter is fixed, its value needs to be included as an additional argument to `gadget_update`. For example, catchability of the indices of smaller sized fish below are estimated using a log linear fit with both slopes and intercepts estimated, but catchabilities for indices of larger sized fish are calculated using log linear fits with fixed slopes. The slopes are fixed to a value of 1, so the argument '`slope = 1`' is included in the `gadget_update` call.

Notice that, unlike the distributional likelihood components, a fleet is not specified for the survey indices, unless `si_type` is set to `'fleets'`, `'effort'`, `'acoustic'`. Therefore, the suitability curve of a certain fleet is not accounted for when catchabilities are estimated, except when `si_type` is set to `'fleets'` or  `'effort'` . In this latter case, a single catchability is estimated instead of a series of catchabilities, corresponding to each age or length index included. Each survey index is matched to stock size observations at a given time and area; therefore, if there are two length- or age-based survey index series with the same time and area, they can each be incorporated as separate likelihood components as long as they have different names. 

```{r}
lik <-
  lik %>%  
  gadget_update("surveyindices",
                name = paste0("si.", names(slices)[1]),
                weight = 1,
                data = SI1,
                fittype = 'loglinearfit',
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("surveyindices",
                name = paste0("si.", names(slices)[2]),
                weight = 1,
                data = SI2,
                fittype = 'loglinearfit',
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("surveyindices",
                name = paste0("si.", names(slices)[3]),
                weight = 1,
                data = SI3,
                fittype = 'fixedslopeloglinearfit',
                slope=1,
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("surveyindices",
                name = paste0("si.", names(slices)[4]),
                weight = 1,
                data = SI4,
                fittype = 'fixedslopeloglinearfit',
                slope=1,
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("surveyindices",
                name = paste0("si.", names(slices)[5]),
                weight = 1,
                data = SI5,
                fittype = 'fixedslopeloglinearfit',
                slope=1,
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("surveyindices",
                name = paste0("si.", names(slices)[6]),
                weight = 1,
                data = SI6,
                fittype = 'fixedslopeloglinearfit',
                slope=1,
                stocknames = c("lingimm","lingmat")) %>% 
  gadget_update("surveyindices",
                name = paste0("si.", names(slices)[7]),
                weight = 1,
                data = SI7,
                fittype = 'fixedslopeloglinearfit',
                slope=1,
                stocknames = c("lingimm","lingmat")) 
lik 
  
```

Finally, there are two components to the likelihood that do not incorporate data but are instead used to ensure that the optimisation search algorithm remains within reasonable ranges of the objective function. The first component provides penalties for exceeding parameter bounds as set by the user in the input parameter file. In this case, the weight is set to 0.5, but see the troubleshooting page for cases in which it may be wise to set this weight higher. The data frame provided to this likelihood component specifies the power and parameter weights supplied if the parameter value exceeds its upper and lower bounds. In this case, if the likelihood of a parameter exceeds either the upper or lower bound, the distance between the bound and the parameter value is squared and multiplied by the parameter weights 10000; then the sum of these scores across all parameters is multiplied by the 0.5 weighting before being summed along with the other likelihood components to form the total. 

Understocking introduces a penalty when there are insufficent prey to met the requiremtnes of the predators. For example, if landings of the fleets exceeds available biomass, then the penalty is applied. The understocking penalty likelihood is then defined as the sum of the number of prey that are 'missing', raised to a power set by default to 2, and multiplied by the likelihood weight of 100 as indicated below.

```{r}
  lik <-
    lik %>% 
    gadget_update("penalty",
                name = "bounds",
                weight = "0.5",
                data = data.frame(
                  switch = c("default"),
                  power = c(2),
                  upperW=10000,
                  lowerW=10000,
                  stringsAsFactors = FALSE)) %>%
    gadget_update("understocking",
                name = "understocking",
                weight = "100")

lik %>% 
  write.gadget.file(gd)

```

Notice that when the likelihood file is written, so are all the supporting aggregation files under the
'Aggfiles' folder.

## Fitting a Gadget model to data 

Now that all data and likelihood components are incorporated, it is possible to run `callGadget` using a fitting procedure. Like any other optimisation routine, what is still needed are 1) starting values for the parameters, and 2) optimisation routine settings.

### Create starting values

If we start here with the same simple simulation as in day 2 of the ling model, we can see that a file called 'params.out' is automatically generated (same timestamp as the simple_log). However, if you open this file, is shows that each parameter was given a value of 1 with bounds set to {-9999, 9999} and no optimisation (0). Therefore, this simulation was run with very unrealistic parameters. Obviously this is not a helpful run in itself, but it is at least useful for conveniently generating an initial parameter file with the correct Gadget structure containing all the parameters specified throughout the Gadget model files. 

```{r}
Sys.setenv(GADGET_WORKING_DIR=normalizePath(gd))
callGadget(s=1,log = 'simple_log') 

read.gadget.parameters(paste0(gd,'/params.out'))
```

Now that the structure is set, it is much easier to change the bounds and starting values to more realistic ones using `init_guess`. This function uses the `grepl` function internally to match names, so that large chunks of parameters with similar names can have their bounds similarly changed.

```{r}
lw_pars <- c(0.00000228, 3.20) #same values as in day 2

## update the input parameters with sane initial guesses
read.gadget.parameters(sprintf('%s/params.out',gd)) %>% 
  init_guess('rec.[0-9]|init.[0-9]',1,0.001,1000,1) %>%
  init_guess('recl',12,4,20,1) %>% 
  init_guess('rec.sd',5, 4, 20,1) %>% 
  init_guess('Linf',160, 100, 200,1) %>% 
  init_guess('k$',90, 40, 100,1) %>% 
  init_guess('bbin',6, 1e-08, 100, 1) %>% 
  init_guess('alpha', 0.5,  0.01, 3, 1) %>% 
  init_guess('l50',50,10,100,1) %>% 
  init_guess('walpha',lw_pars[1], 1e-10, 1,0) %>% 
  init_guess('wbeta',lw_pars[2], 2, 4,0) %>% 
  init_guess('M$',0.15,0.001,1,0) %>% 
  init_guess('rec.scalar',400,1,500,1) %>% 
  init_guess('init.scalar',200,1,300,1) %>% 
  init_guess('mat2',60,0.75*60,1.25*60,1) %>% 
  init_guess('mat1',70,  10, 200, 1) %>% 
  init_guess('init.F',0.4,0.1,1,1) %>% 
  write.gadget.parameters(.,file=sprintf('%s/params.in',gd))

read.gadget.parameters(sprintf('%s/params.in',gd))

```
  
Now is a good point to scan the resulting 'params.in' file to make sure that a parameter was not forgotten and left with default values and bounds.

### Set optimisation routine

To run an optimisiation via callGadget is simple: it simply requires changing the argument `s=1` to `l=1`, removing the log (very important unless you want to crash you computer), providing an input parameter file with starting values (below as `i = 'params.in'`), and providing a file name to which final parameter values should be written (below as `p='params.init'`).

```{r}
timestamp()
callGadget(l=1,i='params.in',p='params.init')
timestamp()

```

Now that reasonable bounds and starting values have been set for an optimisation run, default settings can be used to run a short and simple Gadget optimisation. If you open the params.init file, comments can be found at the top that detail the computer and timing of the run (as in a simulation run), as well as optimisation information (something like 'Hooke & Jeeves algorithm ran for 1039 function evaluations'), the likelihood value at the end '1e+10' and a reason for stopping ('because the maximum number of function evaluations was reached'). You can also see from the timestamps above that this run took roughly 30 seconds, depending on the computer being used. 

Obviously, the default settings for optimisation (1000 function evaluations using the Hooke and Jeeves algorithm), are insufficient to find a reasonable optimum. Gadget can also take quite a while to optimise a model (~ 6 hours for ling on a very fast computer). Therefore, it is best to plan ahead of time on which computer and when a model will be optimised, as well as which optimisation methods will be used. 
  
To change from the default settings, an opimisation file needs to be provided to `callGadget` via the `opt` argument. This is the file we provide for demonstration: it shows that we have implemented 3 consecutive optimisation routines. The first begins with simulated annealing (and parameter settings needed for it as well as a seed), followed by a Hooke and Jeeves algorithm that begins where simmulated annealing leaves off, and finally a BFGS algorithm. The three routines are listed in this manner to consecutively to transition from a broader global search (simulated annealing) to one that provides faster (Hooke and Jeeves) and more precise (BFGS) results, in an attempt to make the search more efficient. These three are the only algorithms currently implemented, but others are in a development phase. Also note that a seed is set in the beginning algorithm: it is good practice to both use random starting values as well as a set seed to check for possible optimisation problems. More information on parameters specific to each algorithm can be found in the Gadget User Guide.

Each algorithm also has a low maximum number of iterations set in the optimisation file below, which is clearly not enough to be realistic: realistic values used for ling are given as notes within the optinfofile and are not read below. However, the each model presents new challenges to the search algorithm, so in some complex cases it may be necessary to rely mostly on simulating annealing, whereas in other more well-defined cases it may be possible to rely mostly on the faster algorithms. At this point in development, speeding Gadget runs is mostly done by using computers with multiple cores and faster processors. 

```{r}
read.gadget.optinfo('data_provided/optinfofile')
```

Here, we have implemented a fitting procedure that begins where the last default optimisation values left off (in the 'params.init' file). The 'params.opt' file will therefore be the result of a longer and more well-defined search as described by the 'optinfofile'.

```{r}
timestamp()
callGadget(l=1,i='params.init',p='params.opt', opt = '../../data_provided/optinfofile')
timestamp()
```



## Using the reiterative function to fit a Gadget statistical model



## Visualize Gadget statistical model fit


